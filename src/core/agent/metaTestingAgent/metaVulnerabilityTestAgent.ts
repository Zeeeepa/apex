/**
 * MetaVulnerabilityTestAgent
 *
 * A cognitive-loop-driven vulnerability testing agent designed to be spawned
 * in parallel by the PentestOrchestrator. Each instance tests ONE (target, vulnerabilityClass) pair.
 *
 * Key patterns (inspired by CyberAutoAgent):
 * - Confidence-driven reasoning (HYPOTHESIS → VALIDATION)
 * - Plans as external working memory with checkpoints
 * - Meta-prompting for runtime optimization
 * - Direct-first economics
 */

import { tool, hasToolCall, type StreamTextOnStepFinishCallback, type ToolSet } from 'ai';
import { z } from 'zod';
import { streamResponse, type AIModel } from '../../ai';
import {
  createPentestTools,
  type ExecuteCommandOpts,
  type ExecuteCommandResult,
  type HttpRequestOpts,
  type HttpRequestResult,
} from '../tools';
import { Logger } from '../logger';
import { Session } from '../../session';
import { join } from 'path';
import { existsSync, mkdirSync, writeFileSync, readdirSync, readFileSync } from 'fs';

// Import tools and types
import type { MetaTestingSessionInfo, CognitiveState } from './types';
import { createPocTool, createDocumentFindingTool } from './pocTools';
import { createPlanMemoryTools, loadPlan, loadAdaptations } from './planMemory';
import { createPromptOptimizerTool } from './promptOptimizer';
import {
  buildSystemPrompt as buildVulnClassPrompt,
  getVulnerabilityClassName,
} from '../orchestrator/prompts';
import type { VulnerabilityClass } from '../orchestrator/types';
import { createAuthBypassTool } from './authBypassAgent';
// import { createStagehandTool } from './browserTools';

const RETRY_CONFIG = {
  maxRetries: 5,
  initialDelayMs: 1000,
  maxDelayMs: 60000,
  backoffMultiplier: 2,
};

function isOverloadedError(error: any): boolean {
  const message = error?.message?.toLowerCase() || '';
  const status = error?.status || error?.statusCode;
  return (
    status === 429 ||
    status === 529 ||
    status === 503 ||
    message.includes('overloaded') ||
    message.includes('rate limit') ||
    message.includes('too many requests') ||
    message.includes('capacity') ||
    message.includes('temporarily unavailable')
  );
}

// this is used to gracefully handle context-length errors
function isContextTooLongError(error: any): boolean {
  const message = error?.message?.toLowerCase() || '';
  const status = error?.status || error?.statusCode;
  return (
    status === 400 &&
    (message.includes('too long') ||
     message.includes('context length') ||
     message.includes('maximum context') ||
     message.includes('input is too long'))
  );
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

function saveAgentMessages(
  sessionRootPath: string,
  agentName: string,
  messages: any[],
  metadata?: Record<string, any>
): string {
  const subagentsDir = join(sessionRootPath, 'subagents');
  if (!existsSync(subagentsDir)) {
    mkdirSync(subagentsDir, { recursive: true });
  }

  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
  const sanitizedName = agentName.toLowerCase().replace(/[^a-z0-9-]/g, '-');
  const filename = `${sanitizedName}-${timestamp}.json`;
  const filepath = join(subagentsDir, filename);

  writeFileSync(filepath, JSON.stringify({
    agentName,
    timestamp: new Date().toISOString(),
    ...metadata,
    messages,
  }, null, 2));

  return filepath;
}

export interface SpawnVulnerabilityTestRequest {
  target: string;
  objective: string;
  vulnerabilityClass: string;
  evidence: string;
  priority: 'critical' | 'high' | 'medium';
}

export interface MetaVulnerabilityTestInput {
  target: string;
  objective: string;
  vulnerabilityClass: VulnerabilityClass;
  authenticationInfo?: {
    method: string;
    details: string;
    credentials?: string;
    cookies?: string;
    headers?: string;
  };
  authenticationInstructions?: string;
  outcomeGuidance: string;
  session: {
    id: string;
    rootPath: string;
    findingsPath: string;
    logsPath: string;
    pocsPath: string;
  };
}

export interface MetaVulnerabilityTestResult {
  vulnerabilitiesFound: boolean;
  findingsCount: number;
  pocPaths: string[];
  findingPaths: string[];
  summary: string;
  error?: string;
}

const COGNITIVE_LOOP_PROMPT = `
## Cognitive Loop (MANDATORY)

### Before EVERY tool call, state:
\`\`\`
HYPOTHESIS:
- Technique: [what I'm trying] (attempt N of method, attempt M of approach)
- Confidence: [0-100%] because [reasoning]
- Expected: if TRUE → [outcome], if FALSE → [pivot plan]
\`\`\`

### After EVERY tool result, state:
\`\`\`
VALIDATION:
- Outcome: [YES/NO + evidence]
- Constraint learned: [specific, not vague]
- Confidence: BEFORE [X%] → AFTER [Y%]
- Decision: [pivot (<50%) | iterate (50-80%) | exploit (>80%)]
\`\`\`

## Confidence Thresholds
- **>80%**: Direct exploitation
- **50-80%**: Hypothesis testing
- **<50%**: MUST pivot to different approach

## Direct-First Economics
Prefer shortest path to objective:
- SQLi found? → UNION extract (3 steps) NOT enumerate schema (20 steps)
- Creds found? → Login (1 step) NOT crack (60 steps)

## CRITICAL: Filter Detection & Bypass Protocol

**When a payload fails, ALWAYS determine if it's due to filtering:**

1. **Detect filtering**: Compare responses with/without special characters
2. **If filtered**: MUST test bypass techniques (encodings, nested patterns, wrappers)
3. **Never conclude "not vulnerable"** until you've tested 3-4 bypass categories
4. **Use mutate_payload tool** to generate encoding variants when filtering detected

## CRITICAL: Reflected Input → Test SSTI Before XSS

**When you find user input reflected in a response:**

1. **Test SSTI payloads FIRST** (leads to RCE - far more severe than XSS)
2. Test: \`{{7*7}}\`, \`<%= 7*7 %>\`, \`\${7*7}\` - look for \`49\` in response
3. **Only if ALL SSTI payloads fail** → then test XSS payloads
4. If \`7*7\` evaluates to \`49\` → SSTI confirmed, escalate to RCE immediately

## Goal Focus Checkpoint

**Periodically ask yourself:**
- Am I working toward the ACTUAL objective?
- Am I being distracted by decoy content (fake flags, suspicious strings)?
- Does my current action directly progress toward the goal?

If distracted → refocus on objective immediately.

## Adaptation Tracking
After significant attempts:
- Call store_adaptation with worked=true/false
- Include constraint_learned if you discovered a blocker
- At 3+ failures, call optimize_prompt to update guidance

## Tool Usage Summary

| Tool | Purpose | When to Use |
|------|---------|-------------|
| execute_command | Run shell commands | Recon, scanning, validation |
| http_request | Make HTTP requests | Direct endpoint testing |
| cve_lookup | Search CVE database for known vulns | When you identify software versions in headers or responses |
| mutate_payload | Encoding/obfuscation | When payloads blocked by WAF or other filters |
| auth_bypass_test | Specialized authorization bypass testing | When you discover auth endpoints - tests privilege escalation and parameter manipulation |
| create_poc | Create and run POC scripts | Validating vulnerabilities |
| document_finding | Record confirmed vulns | After POC confirms exploit |
| store_plan | Save/update strategic plan | Step 0 and checkpoints |
| get_plan | Retrieve current plan | At checkpoints |
| store_adaptation | Record approach outcome | After each technique attempt |
| optimize_prompt | Update execution guidance | After failures, at checkpoints |
| spawn_vulnerability_test | Spawn new agent for different vuln class | When you find indicators of a DIFFERENT vulnerability type |

## When to Spawn New Vulnerability Tests

**If you discover indicators of a vulnerability class DIFFERENT from your current assignment:**

1. **Evaluate the evidence**: Is it a strong indicator? (error messages, unexpected behaviors, technology hints)
2. **Decide: SPAWN or PIVOT**
   - SPAWN: Both vulnerabilities worth testing → use spawn_vulnerability_test, continue current testing
   - PIVOT: Discovered vulnerability clearly more important → switch focus entirely (no spawn needed)
3. **For SPAWN**: Call spawn_vulnerability_test with the discovered vulnerability class and evidence

**Priority guidance for spawning:**
- critical: Potential for RCE or full system compromise
- high: Potential for sensitive data access
- medium: Other security issues`;

/**
 * Run the MetaVulnerabilityTestAgent
 *
 * Tests a single (target, vulnerabilityClass) pair using cognitive loop reasoning.
 */
export async function runMetaVulnerabilityTestAgent(
  opts: {
    input: MetaVulnerabilityTestInput;
    model: AIModel;
    remoteSandboxUrl?: string;
    onStepFinish?: StreamTextOnStepFinishCallback<ToolSet>;
    abortSignal?: AbortSignal;
    toolOverride?: {
      execute_command?: (opts: ExecuteCommandOpts) => Promise<ExecuteCommandResult>;
      http_request?: (opts: HttpRequestOpts) => Promise<HttpRequestResult>;
    };
    onSpawnAgent?: (request: SpawnVulnerabilityTestRequest) => void;
  }
): Promise<MetaVulnerabilityTestResult> {
  const { input, model, remoteSandboxUrl, onStepFinish, abortSignal, toolOverride, onSpawnAgent } = opts;
  const {
    target,
    objective,
    vulnerabilityClass,
    authenticationInfo,
    authenticationInstructions,
    outcomeGuidance,
    session,
  } = input;

  const logger = new Logger(
    { id: session.id, rootPath: session.rootPath, logsPath: session.logsPath } as Session.SessionInfo,
    `meta-vuln-test-${vulnerabilityClass}.log`
  );

  logger.info(`Starting MetaVulnerabilityTestAgent for ${target} [${vulnerabilityClass}]`);
  logger.info(`Objective: ${objective}`);

  if (!existsSync(session.pocsPath)) {
    mkdirSync(session.pocsPath, { recursive: true });
  }

  const sessionInfo: MetaTestingSessionInfo = {
    id: session.id,
    rootPath: session.rootPath,
    findingsPath: session.findingsPath,
    logsPath: session.logsPath,
    pocsPath: session.pocsPath,
  };

  const sessionForTools = {
    ...session,
    targets: [target],
  } as Session.SessionInfo;

  const { 
    execute_command,
    http_request,
    fuzz_endpoint,
    smart_enumerate,
    cve_lookup,
    mutate_payload,
  } = createPentestTools(
    sessionForTools,
    undefined,
    toolOverride,
    undefined,  // onTokenUsage - not needed here
    abortSignal
  );

  const { create_poc, pocPaths } = createPocTool(sessionInfo, logger, toolOverride);
  const { document_finding, findingPaths } = createDocumentFindingTool(sessionInfo, logger, target);
  const { store_plan, get_plan, store_adaptation } = createPlanMemoryTools(sessionInfo, logger);
  const { optimize_prompt } = createPromptOptimizerTool(sessionInfo, logger);
  const auth_bypass_test = createAuthBypassTool(sessionInfo, logger, model, toolOverride);

  // const use_browser = await createStagehandTool(sessionInfo, logger, remoteSandboxUrl ? remoteSandboxUrl : target);

  const spawn_vulnerability_test = tool({
    description: `Spawn a NEW vulnerability test agent for a DIFFERENT vulnerability class than your current assignment.

Use this when you discover strong indicators of a vulnerability type you're NOT currently testing.
The new agent will run in parallel while you continue your current testing.

When to use:
- Found error message suggesting different vuln (e.g., "unserialize", "phar://", "template error")
- Discovered behavior indicating different attack vector
- Found endpoint that clearly needs different testing methodology
- **CRYPTO**: Error mentions "Invalid IV", "padding", "decryption", "cipher", "CBC", "AES" - spawn crypto agent

Decision: SPAWN vs PIVOT
- SPAWN (use this tool): Continue your current testing AND start new agent for discovered vuln
- PIVOT (don't use this, just switch focus): Abandon current testing to focus entirely on discovered vuln

Use SPAWN when both are worth testing. Use PIVOT when discovered vuln is clearly more important.`,
    inputSchema: z.object({
      vulnerabilityClass: z.string().describe('Vulnerability class to test: deserialization, ssti, sqli, xss, lfi, ssrf, xxe, command-injection, idor, crypto'),
      objective: z.string().describe('Specific testing objective for the new agent'),
      evidence: z.string().describe('Evidence that led to this discovery (error message, behavior, etc.)'),
      priority: z.enum(['critical', 'high', 'medium']).describe('Priority: critical=RCE potential, high=data access, medium=other'),
    }),
    execute: async (params) => {
      if (!onSpawnAgent) {
        // If no spawn callback, save to file for later processing
        const spawnDir = join(session.rootPath, 'spawn-requests');
        if (!existsSync(spawnDir)) {
          mkdirSync(spawnDir, { recursive: true });
        }
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
        const spawnFile = join(spawnDir, `spawn-${params.vulnerabilityClass}-${timestamp}.json`);
        writeFileSync(spawnFile, JSON.stringify({
          ...params,
          target,
          discoveredBy: vulnerabilityClass,
          timestamp: new Date().toISOString(),
        }, null, 2));
        logger.info(`Spawn request saved (no callback): ${params.vulnerabilityClass} for ${target}`);
        return {
          success: true,
          spawned: false,
          message: `Spawn request recorded for ${params.vulnerabilityClass} testing. Will be processed after current testing completes.`,
        };
      }

      // Call the spawn callback to queue new agent
      onSpawnAgent({
        target,
        objective: params.objective,
        vulnerabilityClass: params.vulnerabilityClass,
        evidence: params.evidence,
        priority: params.priority,
      });

      logger.info(`Spawned new agent: ${params.vulnerabilityClass} for ${target}`);
      return {
        success: true,
        spawned: true,
        message: `New ${params.vulnerabilityClass} test agent spawned for ${target}. Continuing current ${vulnerabilityClass} testing.`,
      };
    },
  });

  let testingSummary = '';
  const complete_testing = tool({
    description: `Signal that vulnerability testing is complete.

Call when:
- Vulnerability found and documented
- Max attempts exhausted
- Target not vulnerable to this class
- Confidence dropped and pivoted through all approaches`,
    inputSchema: z.object({
      summary: z.string().describe('Summary of testing and results'),
      vulnerabilitiesFound: z.boolean().describe('Whether vulnerabilities were confirmed'),
    }),
    execute: async (result) => {
      testingSummary = result.summary;
      logger.info(`Testing complete: ${result.vulnerabilitiesFound ? 'Found' : 'None'}`);
      return { success: true, message: 'Testing completed.' };
    },
  });

  const tools = {
    execute_command,
    http_request,
    fuzz_endpoint,
    // smart_enumerate,
    cve_lookup,
    auth_bypass_test,
    mutate_payload,
    create_poc,
    document_finding,
    store_plan,
    get_plan,
    store_adaptation,
    optimize_prompt,
    spawn_vulnerability_test,
    complete_testing,
    // use_browser
  };

  const vulnClassPrompt = buildVulnClassPrompt(vulnerabilityClass, outcomeGuidance);
  const systemPrompt = vulnClassPrompt + '\n\n---\n' + COGNITIVE_LOOP_PROMPT;

  const userPrompt = buildUserPrompt({
    target,
    objective,
    vulnerabilityClass,
    authenticationInfo,
    authenticationInstructions,
  });

  let attempt = 0;
  let lastError: any = null;

  while (attempt < RETRY_CONFIG.maxRetries) {
    attempt++;

    try {
      logger.info(`Running agent (attempt ${attempt}/${RETRY_CONFIG.maxRetries})`);

      const streamResult = streamResponse({
        prompt: userPrompt,
        system: systemPrompt,
        model,
        tools,
        onStepFinish,
        stopWhen: hasToolCall('complete_testing'),
        abortSignal,
        silent: true,
      });

      // Consume stream
      for await (const chunk of streamResult.fullStream) {
        if (chunk.type === 'error') {
          const error = (chunk as any).error;
          if (isOverloadedError(error)) {
            throw error;
          }
        }
      }

      logger.info(`Testing finished. Findings: ${findingPaths.length}`);

      try {
        const response = await streamResult.response;
        if (response.messages?.length > 0) {
          const sanitizedTarget = target.replace(/[^a-z0-9]/gi, '-').substring(0, 30);
          saveAgentMessages(
            session.rootPath,
            `meta-vuln-${vulnerabilityClass}-${sanitizedTarget}`,
            response.messages,
            { target, objective, vulnerabilityClass, findingsCount: findingPaths.length }
          );
        }
      } catch (e: any) {
        logger.error(`Failed to save messages: ${e.message}`);
      }

      return {
        vulnerabilitiesFound: findingPaths.length > 0,
        findingsCount: findingPaths.length,
        pocPaths,
        findingPaths,
        summary: testingSummary || `Tested ${target} for ${getVulnerabilityClassName(vulnerabilityClass)}`,
      };

    } catch (error: any) {
      lastError = error;

      if (isOverloadedError(error) && attempt < RETRY_CONFIG.maxRetries) {
        const delay = Math.min(
          RETRY_CONFIG.initialDelayMs * Math.pow(RETRY_CONFIG.backoffMultiplier, attempt - 1),
          RETRY_CONFIG.maxDelayMs
        );
        logger.info(`API overloaded, retrying in ${delay / 1000}s...`);
        await sleep(delay);
        continue;
      }

      if (isContextTooLongError(error)) {
        logger.error(`Context too long for ${target} [${vulnerabilityClass}]: ${error.message}`);
        return {
          vulnerabilitiesFound: false,
          findingsCount: 0,
          pocPaths: [],
          findingPaths: [],
          summary: `Testing terminated: Context exceeded model limits for ${getVulnerabilityClassName(vulnerabilityClass)}`,
          error: `Context too long: ${error.message}`,
        };
      }

      throw error;
    }
  }

  logger.error(`Failed after ${RETRY_CONFIG.maxRetries} attempts: ${lastError?.message}`);
  throw new Error(`Max retries exhausted. Last error: ${lastError?.message}`);
}

function buildUserPrompt(params: {
  target: string;
  objective: string;
  vulnerabilityClass: VulnerabilityClass;
  authenticationInfo?: MetaVulnerabilityTestInput['authenticationInfo'];
  authenticationInstructions?: string;
}): string {
  const { target, objective, vulnerabilityClass, authenticationInfo, authenticationInstructions } = params;

  let prompt = `# Testing Assignment

**Target:** ${target}
**Objective:** ${objective}
**Vulnerability Class:** ${getVulnerabilityClassName(vulnerabilityClass)}

## Your Task

Test for ${getVulnerabilityClassName(vulnerabilityClass)} vulnerabilities using the cognitive loop:

1. **Create initial plan** with store_plan (phases: Recon, Test, Exploit)
2. **Form hypothesis** before each action (state confidence 0-100%)
3. **Validate results** after each action (update confidence)
4. **Create POC** when confidence >80% (bash preferred)
5. **Document finding** after successful POC
6. **Track adaptations** (store_adaptation for meta-prompting)
7. **Complete** by calling complete_testing

`;

  if (authenticationInstructions) {
    prompt += `## Authentication Instructions

${authenticationInstructions}

**Include authentication in all POC scripts.**
`;
  }

  if (authenticationInfo) {
    prompt += `## Authentication Details

- Method: ${authenticationInfo.method}
- Details: ${authenticationInfo.details}
${authenticationInfo.cookies ? `- Cookies: ${authenticationInfo.cookies}` : ''}
${authenticationInfo.headers ? `- Headers: ${authenticationInfo.headers}` : ''}
`;
  }

  prompt += `
## Critical Rules

- HYPOTHESIS before every tool call
- VALIDATION after every result
- Pivot at <50% confidence
- Direct-first economics (minimal steps)
- Max 3 POC attempts per approach

Begin with your initial plan, then start testing.
`;

  return prompt.trim();
}
